#In the first part of webscraping we will look for all stocks available on the webpage. 
#Once we have the list of stocks and their subsites, we look for turnover value. The turnover value changes depending on the timeframe.
#In example below, we are interested in the turnover based on the data from the last year - 1Y

# imports of esssential libraries
import requests
import re
from selenium import webdriver
from selenium.webdriver.common.action_chains import ActionChains
import pandas as pd

# access the subsite consisting the list of all listed equities in Poland
request=requests.get('https://www.bankier.pl/gielda/notowania/akcje')

#the list of names of equities
namelist=[] 
#the list of tickers of equities
tickerlist=[]
webpage_text = request.text

#  remember that the split will give you a list of the whole webpage separated by olWalor textNowrap - you can
#easily iterate throught the page thanks to that   
results = webpage_text.split('''colWalor textNowrap">
        <a title''') 
for item in results[1:]: 
    x=item.split('href')[0]
    x=x[2:(len(x)-2)]
    namelist.append(x)
for item in results[1:]:
    x=item.split('symbol=')[1]
    x=x.split('"')[0]
    tickerlist.append(x)

#find links to the companies' subsites using links starting with a number or a letter
links=re.findall(r'href="/inwestowanie/profile/quote.html\?symbol=[0-9-A-Z]*', request.text)
links = [x for x in links if x != 'href="/inwestowanie/profile/quote.html?symbol='] #cleanse the data from empty links

url = 'https://www.bankier.pl/inwestowanie/profile/quote.html?symbol={}'.format(tickerlist[0])

#initiate chrome browser:
browser=webdriver.Chrome() #important: you need to have a webdriver.exe in the folder of this script
def get_turnover(url):
    browser.get(url)
    browser.implicitly_wait(20) # seconds to load the page (not necessary but useful in case of extremely slow broadband)
    try:      
    
        #once the page loads, a cookie policy request pops up and blocks access to certain elements of the page
        #in order to go further, one needs to click one of the buttons
        popup = browser.find_elements_by_class_name("qc-cmp-button")
        
        #we have the popup element, now we use Actionchain to to perform a Click
        actions = ActionChains(browser)
        actions.click(popup[0]) #popup is a list of popups
        actions.perform()
    except IndexError:
        pass
    button=browser.find_elements_by_xpath("//*[contains(text(), '1R')]") # click the button in order to have Javascript recalculate
    
    #values for the last year (see the website to fully understand the purpose of clicking the button).
    #important! - If thereâ€™s more than one element that matches the query, then only the first will be returned.
    #If nothing can be found, a NoSuchElementException will be raised.
    #unfortunately, our button is beyond the page view. Buttons tend to be clickable only
    #if they are visible, hence we navigate to the button
    ActionChains(browser).move_to_element(button[0]).perform() 
    
    #in my case moving to the button was not enough so scroll down even further
    browser.execute_script("window.scrollTo(0, 500)")
    actions.click(button[0])
    actions.perform()
    elem=browser.find_element_by_class_name('turnoverAverageValue')
    
    #print(elem.is_displayed()) #if the element is displayed, sourcing the data from it is simple.
    #If the element is not displayed, you need the trick from the below:
    return elem.get_attribute('innerText') #attribute innertext is easy to miss when starting to web scrape - IMPORTANT
turnoverlist=[]

#we loop through the tickerlist and use our function get_turnover:
for item in tickerlist:
    url = 'https://www.bankier.pl/inwestowanie/profile/quote.html?symbol={}'.format(item)
    turnoverlist.append(get_turnover(url))
    
#we create a dataframe that is later saved in a convenient Excel spreadsheet form
df=pd.DataFrame({'Namelist': namelist, 'Ticker':tickerlist,'AverageTurnover':turnoverlist})
df.to_excel("C:\Python\output.xlsx")
